{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "news_sentiment_classification_updown_lstm.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2u7hK3dACijMg2YLkZyjR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmsk98/Graduation/blob/main/news_sentiment_classification_updown_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ZoKG_62psF"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDaNHrXP2vCe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEIIWY07nqf5"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXI0QMukP4II"
      },
      "source": [
        "##현대차\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "!pip install konlpy\n",
        "# 불용어 로드\n",
        "url = 'https://raw.githubusercontent.com/chaerui7967/stock_predict_news_and_youtube/master/Sentiment_Analysis/data/stopwords_ver1.txt'\n",
        "stopwords = list(pd.read_csv(url, header=None)[0])\n",
        "\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/뉴스토픽분류/')\n",
        "os.getcwd()\n",
        "\n",
        "train =pd.read_csv('hyundai_community_train_set.csv')\n",
        "test=pd.read_csv('hyundai_community_test_set.csv')\n",
        "\n",
        "\n",
        "\n",
        "train['updown_label'] = None\n",
        "test['updown_label'] = None\n",
        "\n",
        "for i in range(len(train)):\n",
        "    if train['Change'][i] > 0 :\n",
        "        train['updown_label'][i] = 1\n",
        "    else:\n",
        "        train['updown_label'][i]= 0\n",
        "    \n",
        "\n",
        "for i in range(len(test)):\n",
        "    if test['Change'][i] >0:\n",
        "        test['updown_label'][i] =1\n",
        "    else:\n",
        "        test['updown_label'][i] =0\n",
        "        \n",
        "for i in range(len(train)):\n",
        "    train['text'][i] = str(train['title'][i])\n",
        "    \n",
        "for i in range(len(test)):\n",
        "    test['text'][i] = str(test['title'][i])\n",
        "    \n",
        "\n",
        "\n",
        "import konlpy\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "X_train = [] \n",
        "for sentence in train['text']: \n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_train.append(temp_X) \n",
        "\n",
        "X_test = [] \n",
        "for sentence in test['text']:\n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords]\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_test.append(temp_X)\n",
        "    \n",
        "    \n",
        "import numpy as np \n",
        "y_train = [] \n",
        "y_test = [] \n",
        "\n",
        "for i in range(len(train['updown_label'])): \n",
        "    if train['updown_label'].iloc[i] == 1: \n",
        "        y_train.append(1) \n",
        "    elif train['updown_label'].iloc[i] ==0:\n",
        "        y_train.append(0) \n",
        "\n",
        "\n",
        "for i in range(len(test['updown_label'])): \n",
        "    if test['updown_label'].iloc[i] == 1: \n",
        "        y_test.append(1) \n",
        "    elif test['updown_label'].iloc[i] == 0: \n",
        "        y_test.append(0) \n",
        "\n",
        "\n",
        "y_train = np.array(y_train) \n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "max_words = 35000\n",
        "\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train.text),\n",
        "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test.text),\n",
        "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)\n",
        "\n",
        "from keras.layers import Embedding, Dense, LSTM \n",
        "from keras.models import Sequential \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Embedding(max_words, 100)) \n",
        "model.add(LSTM(128)) \n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_accuracy', patience=50)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        epochs=100, batch_size=10, validation_split=0.1,\n",
        "                        callbacks=[earlystopping])\n",
        "    \n",
        "#def decode_sentiment(score):\n",
        "#    return 1 if score>0.5 else 0\n",
        "def decode_sentiment(score):\n",
        "    if score > 0.75 :\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]\n",
        "\n",
        "test['predict_lstm'] = None\n",
        "\n",
        "for i in range(len(test)):\n",
        "    test['predict_lstm'][i] = y_pred_1d[i]\n",
        "    \n",
        "    \n",
        "test['predict_lstm'].value_counts()\n",
        "\n",
        "label = test['label']\n",
        "pred = test['predict_lstm']\n",
        "\n",
        "stock_price =test[['date','Open','High','Low','Close','Volume','predict_lstm']]\n",
        "\n",
        "stock_price=stock_price.drop_duplicates(['date'],keep='first')\n",
        "\n",
        "stock_price = stock_price.reset_index(drop=True)\n",
        "\n",
        "stock_price.to_csv('hyundai_community_test_2020_lstm_updown.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJNpxao8P7Uo"
      },
      "source": [
        "##sk\n",
        "train =pd.read_csv('sk_community_train_set.csv')\n",
        "test=pd.read_csv('sk_community_test_set.csv')\n",
        "\n",
        "\n",
        "\n",
        "train['updown_label'] = None\n",
        "test['updown_label'] = None\n",
        "\n",
        "for i in range(len(train)):\n",
        "    if train['Change'][i] > 0 :\n",
        "        train['updown_label'][i] = 1\n",
        "    else:\n",
        "        train['updown_label'][i]= 0\n",
        "    \n",
        "\n",
        "for i in range(len(test)):\n",
        "    if test['Change'][i] >0:\n",
        "        test['updown_label'][i] =1\n",
        "    else:\n",
        "        test['updown_label'][i] =0\n",
        "        \n",
        "for i in range(len(train)):\n",
        "    train['text'][i] = str(train['title'][i])\n",
        "    \n",
        "for i in range(len(test)):\n",
        "    test['text'][i] = str(test['title'][i])\n",
        "    \n",
        "\n",
        "\n",
        "import konlpy\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "X_train = [] \n",
        "for sentence in train['text']: \n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_train.append(temp_X) \n",
        "\n",
        "X_test = [] \n",
        "for sentence in test['text']:\n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords]\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_test.append(temp_X)\n",
        "    \n",
        "    \n",
        "import numpy as np \n",
        "y_train = [] \n",
        "y_test = [] \n",
        "\n",
        "for i in range(len(train['updown_label'])): \n",
        "    if train['updown_label'].iloc[i] == 1: \n",
        "        y_train.append(1) \n",
        "    elif train['updown_label'].iloc[i] ==0:\n",
        "        y_train.append(0) \n",
        "\n",
        "\n",
        "for i in range(len(test['updown_label'])): \n",
        "    if test['updown_label'].iloc[i] == 1: \n",
        "        y_test.append(1) \n",
        "    elif test['updown_label'].iloc[i] == 0: \n",
        "        y_test.append(0) \n",
        "\n",
        "\n",
        "y_train = np.array(y_train) \n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "max_words = 35000\n",
        "\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train.text),\n",
        "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test.text),\n",
        "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)\n",
        "\n",
        "from keras.layers import Embedding, Dense, LSTM \n",
        "from keras.models import Sequential \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Embedding(max_words, 100)) \n",
        "model.add(LSTM(128)) \n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_accuracy', patience=50)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        epochs=100, batch_size=10, validation_split=0.1,\n",
        "                        callbacks=[earlystopping])\n",
        "    \n",
        "#def decode_sentiment(score):\n",
        "#    return 1 if score>0.5 else 0\n",
        "def decode_sentiment(score):\n",
        "    if score > 0.75 :\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]\n",
        "\n",
        "test['predict_lstm'] = None\n",
        "\n",
        "for i in range(len(test)):\n",
        "    test['predict_lstm'][i] = y_pred_1d[i]\n",
        "    \n",
        "    \n",
        "test['predict_lstm'].value_counts()\n",
        "\n",
        "label = test['label']\n",
        "pred = test['predict_lstm']\n",
        "\n",
        "stock_price =test[['date','Open','High','Low','Close','Volume','predict_lstm']]\n",
        "\n",
        "stock_price=stock_price.drop_duplicates(['date'],keep='first')\n",
        "\n",
        "stock_price = stock_price.reset_index(drop=True)\n",
        "\n",
        "stock_price.to_csv('sk_community_test_2020_lstm_updown.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WR7mDVzP9j4"
      },
      "source": [
        "##celltrion\n",
        "\n",
        "train =pd.read_csv('celltrion_community_train_set.csv')\n",
        "test=pd.read_csv('celltrion_community_test_set.csv')\n",
        "\n",
        "\n",
        "\n",
        "train['updown_label'] = None\n",
        "test['updown_label'] = None\n",
        "\n",
        "for i in range(len(train)):\n",
        "    if train['Change'][i] > 0 :\n",
        "        train['updown_label'][i] = 1\n",
        "    else:\n",
        "        train['updown_label'][i]= 0\n",
        "    \n",
        "\n",
        "for i in range(len(test)):\n",
        "    if test['Change'][i] >0:\n",
        "        test['updown_label'][i] =1\n",
        "    else:\n",
        "        test['updown_label'][i] =0\n",
        "        \n",
        "for i in range(len(train)):\n",
        "    train['text'][i] = str(train['title'][i])\n",
        "    \n",
        "for i in range(len(test)):\n",
        "    test['text'][i] = str(test['title'][i])\n",
        "    \n",
        "\n",
        "\n",
        "import konlpy\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "X_train = [] \n",
        "for sentence in train['text']: \n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_train.append(temp_X) \n",
        "\n",
        "X_test = [] \n",
        "for sentence in test['text']:\n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords]\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_test.append(temp_X)\n",
        "    \n",
        "    \n",
        "import numpy as np \n",
        "y_train = [] \n",
        "y_test = [] \n",
        "\n",
        "for i in range(len(train['updown_label'])): \n",
        "    if train['updown_label'].iloc[i] == 1: \n",
        "        y_train.append(1) \n",
        "    elif train['updown_label'].iloc[i] ==0:\n",
        "        y_train.append(0) \n",
        "\n",
        "\n",
        "for i in range(len(test['updown_label'])): \n",
        "    if test['updown_label'].iloc[i] == 1: \n",
        "        y_test.append(1) \n",
        "    elif test['updown_label'].iloc[i] == 0: \n",
        "        y_test.append(0) \n",
        "\n",
        "\n",
        "y_train = np.array(y_train) \n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "max_words = 35000\n",
        "\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train.text),\n",
        "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test.text),\n",
        "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)\n",
        "\n",
        "from keras.layers import Embedding, Dense, LSTM \n",
        "from keras.models import Sequential \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Embedding(max_words, 100)) \n",
        "model.add(LSTM(128)) \n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_accuracy', patience=50)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        epochs=100, batch_size=10, validation_split=0.1,\n",
        "                        callbacks=[earlystopping])\n",
        "    \n",
        "#def decode_sentiment(score):\n",
        "#    return 1 if score>0.5 else 0\n",
        "def decode_sentiment(score):\n",
        "    if score > 0.75 :\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]\n",
        "\n",
        "test['predict_lstm'] = None\n",
        "\n",
        "for i in range(len(test)):\n",
        "    test['predict_lstm'][i] = y_pred_1d[i]\n",
        "    \n",
        "    \n",
        "test['predict_lstm'].value_counts()\n",
        "\n",
        "label = test['label']\n",
        "pred = test['predict_lstm']\n",
        "\n",
        "stock_price =test[['date','Open','High','Low','Close','Volume','predict_lstm']]\n",
        "\n",
        "stock_price=stock_price.drop_duplicates(['date'],keep='first')\n",
        "\n",
        "stock_price = stock_price.reset_index(drop=True)\n",
        "\n",
        "stock_price.to_csv('celltrion_community_test_2020_lstm_updown.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DwgWCVOP_nH"
      },
      "source": [
        "##samsung\n",
        "\n",
        "train =pd.read_csv('samsung_community_train_set.csv')\n",
        "test=pd.read_csv('samsung_community_test_set.csv')\n",
        "\n",
        "\n",
        "\n",
        "train['updown_label'] = None\n",
        "test['updown_label'] = None\n",
        "\n",
        "for i in range(len(train)):\n",
        "    if train['Change'][i] > 0 :\n",
        "        train['updown_label'][i] = 1\n",
        "    else:\n",
        "        train['updown_label'][i]= 0\n",
        "    \n",
        "\n",
        "for i in range(len(test)):\n",
        "    if test['Change'][i] >0:\n",
        "        test['updown_label'][i] =1\n",
        "    else:\n",
        "        test['updown_label'][i] =0\n",
        "        \n",
        "for i in range(len(train)):\n",
        "    train['text'][i] = str(train['title'][i])\n",
        "    \n",
        "for i in range(len(test)):\n",
        "    test['text'][i] = str(test['title'][i])\n",
        "    \n",
        "\n",
        "\n",
        "import konlpy\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "X_train = [] \n",
        "for sentence in train['text']: \n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_train.append(temp_X) \n",
        "\n",
        "X_test = [] \n",
        "for sentence in test['text']:\n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords]\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_test.append(temp_X)\n",
        "    \n",
        "    \n",
        "import numpy as np \n",
        "y_train = [] \n",
        "y_test = [] \n",
        "\n",
        "for i in range(len(train['updown_label'])): \n",
        "    if train['updown_label'].iloc[i] == 1: \n",
        "        y_train.append(1) \n",
        "    elif train['updown_label'].iloc[i] ==0:\n",
        "        y_train.append(0) \n",
        "\n",
        "\n",
        "for i in range(len(test['updown_label'])): \n",
        "    if test['updown_label'].iloc[i] == 1: \n",
        "        y_test.append(1) \n",
        "    elif test['updown_label'].iloc[i] == 0: \n",
        "        y_test.append(0) \n",
        "\n",
        "\n",
        "y_train = np.array(y_train) \n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "max_words = 35000\n",
        "\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train.text),\n",
        "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test.text),\n",
        "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)\n",
        "\n",
        "from keras.layers import Embedding, Dense, LSTM \n",
        "from keras.models import Sequential \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Embedding(max_words, 100)) \n",
        "model.add(LSTM(128)) \n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_accuracy', patience=50)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        epochs=100, batch_size=10, validation_split=0.1,\n",
        "                        callbacks=[earlystopping])\n",
        "    \n",
        "#def decode_sentiment(score):\n",
        "#    return 1 if score>0.5 else 0\n",
        "def decode_sentiment(score):\n",
        "    if score > 0.75 :\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]\n",
        "\n",
        "test['predict_lstm'] = None\n",
        "\n",
        "for i in range(len(test)):\n",
        "    test['predict_lstm'][i] = y_pred_1d[i]\n",
        "    \n",
        "    \n",
        "test['predict_lstm'].value_counts()\n",
        "\n",
        "label = test['label']\n",
        "pred = test['predict_lstm']\n",
        "\n",
        "stock_price =test[['date','Open','High','Low','Close','Volume','predict_lstm']]\n",
        "\n",
        "stock_price=stock_price.drop_duplicates(['date'],keep='first')\n",
        "\n",
        "stock_price = stock_price.reset_index(drop=True)\n",
        "\n",
        "stock_price.to_csv('samsung_community_test_2020_lstm_updown.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGnXAs5yQCJd"
      },
      "source": [
        "##lg\n",
        "\n",
        "train =pd.read_csv('lg_community_train_set.csv')\n",
        "test=pd.read_csv('lg_community_test_set.csv')\n",
        "\n",
        "\n",
        "train['updown_label'] = None\n",
        "test['updown_label'] = None\n",
        "\n",
        "for i in range(len(train)):\n",
        "    if train['Change'][i] > 0 :\n",
        "        train['updown_label'][i] = 1\n",
        "    else:\n",
        "        train['updown_label'][i]= 0\n",
        "    \n",
        "\n",
        "for i in range(len(test)):\n",
        "    if test['Change'][i] >0:\n",
        "        test['updown_label'][i] =1\n",
        "    else:\n",
        "        test['updown_label'][i] =0\n",
        "        \n",
        "for i in range(len(train)):\n",
        "    train['text'][i] = str(train['title'][i])\n",
        "    \n",
        "for i in range(len(test)):\n",
        "    test['text'][i] = str(test['title'][i])\n",
        "    \n",
        "\n",
        "\n",
        "import konlpy\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "X_train = [] \n",
        "for sentence in train['text']: \n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_train.append(temp_X) \n",
        "\n",
        "X_test = [] \n",
        "for sentence in test['text']:\n",
        "    temp_X = [] \n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화 \n",
        "    temp_X = [word for word in temp_X if not word in stopwords]\n",
        "    temp_X = [word for word in temp_X if len(word) > 1] # 불용어 제거 \n",
        "    X_test.append(temp_X)\n",
        "    \n",
        "    \n",
        "import numpy as np \n",
        "y_train = [] \n",
        "y_test = [] \n",
        "\n",
        "for i in range(len(train['updown_label'])): \n",
        "    if train['updown_label'].iloc[i] == 1: \n",
        "        y_train.append(1) \n",
        "    elif train['updown_label'].iloc[i] ==0:\n",
        "        y_train.append(0) \n",
        "\n",
        "\n",
        "for i in range(len(test['updown_label'])): \n",
        "    if test['updown_label'].iloc[i] == 1: \n",
        "        y_test.append(1) \n",
        "    elif test['updown_label'].iloc[i] == 0: \n",
        "        y_test.append(0) \n",
        "\n",
        "\n",
        "y_train = np.array(y_train) \n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "max_words = 35000\n",
        "\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train.text),\n",
        "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test.text),\n",
        "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)\n",
        "\n",
        "from keras.layers import Embedding, Dense, LSTM \n",
        "from keras.models import Sequential \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Embedding(max_words, 100)) \n",
        "model.add(LSTM(128)) \n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_accuracy', patience=50)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        epochs=100, batch_size=10, validation_split=0.1,\n",
        "                        callbacks=[earlystopping])\n",
        "    \n",
        "#def decode_sentiment(score):\n",
        "#    return 1 if score>0.5 else 0\n",
        "def decode_sentiment(score):\n",
        "    if score > 0.75 :\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]\n",
        "\n",
        "test['predict_lstm'] = None\n",
        "\n",
        "for i in range(len(test)):\n",
        "    test['predict_lstm'][i] = y_pred_1d[i]\n",
        "    \n",
        "    \n",
        "test['predict_lstm'].value_counts()\n",
        "\n",
        "label = test['label']\n",
        "pred = test['predict_lstm']\n",
        "\n",
        "stock_price =test[['date','Open','High','Low','Close','Volume','predict_lstm']]\n",
        "\n",
        "stock_price=stock_price.drop_duplicates(['date'],keep='first')\n",
        "\n",
        "stock_price = stock_price.reset_index(drop=True)\n",
        "\n",
        "stock_price.to_csv('lg_community_test_2020_lstm_updown.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}